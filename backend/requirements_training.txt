# Additional requirements for training/fine-tuning LLM
transformers>=4.35.0
datasets>=2.14.0
accelerate>=0.24.0
peft>=0.6.0  # For LoRA fine-tuning (optional, more efficient)
bitsandbytes>=0.41.0  # For 8-bit training (optional, saves memory)

